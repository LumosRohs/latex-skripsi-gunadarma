@ARTICLE{abbas2021,
  author={Abbass, Hussein},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Editorial: What is Artificial Intelligence?}, 
  year={2021},
  volume={2},
  number={2},
  pages={94-95},
  keywords={},
  doi={10.1109/TAI.2021.3096243}
}


@misc{alom2018,
      title={The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches}, 
      author={Md Zahangir Alom and Tarek M. Taha and Christopher Yakopcic and Stefan Westberg and Paheding Sidike and Mst Shamima Nasrin and Brian C Van Esesn and Abdul A S. Awwal and Vijayan K. Asari},
      year={2018},
      eprint={1803.01164},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1803.01164}, 
}

@book{geron2019,
  title={Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow},
  author={Aurélien Géron},
  year={2019},
  edition={2nd},
  publisher={O'Reilly Media, Inc.},
  isbn={9781492032649}
}

@article{karthik2022,
  author    = {Karthik, R. and Vaichole, T. S. and Kulkarni, S. K. and Yadav, O. and Khan, F.},
  title     = {Eff2Net: An Efficient Channel Attention-based Convolutional Neural Network for Skin Disease Classification},
  journal   = {Biomed Signal Process Control},
  volume    = {73},
  year      = {2022},
  doi       = {10.1016/j.bspc.2021.103406}
}

@book{kelleher2019,
    author = {Kelleher, John D.},
    title = "{Deep Learning}",
    publisher = {The MIT Press},
    year = {2019},
    month = {09},
    abstract = "{An accessible introduction to the artificial intelligence technology that enables computer vision, speech recognition, machine translation, and driverless cars.Deep learning is an artificial intelligence technology that enables computer vision, speech recognition in mobile phones, machine translation, AI games, driverless cars, and other applications. When we use consumer products from Google, Microsoft, Facebook, Apple, or Baidu, we are often interacting with a deep learning system. In this volume in the MIT Press Essential Knowledge series, computer scientist John Kelleher offers an accessible and concise but comprehensive introduction to the fundamental technology at the heart of the artificial intelligence revolution.Kelleher explains that deep learning enables data-driven decisions by identifying and extracting patterns from large datasets; its ability to learn from complex data makes deep learning ideally suited to take advantage of the rapid growth in big data and computational power. Kelleher also explains some of the basic concepts in deep learning, presents a history of advances in the field, and discusses the current state of the art. He describes the most important deep learning architectures, including autoencoders, recurrent neural networks, and long short-term networks, as well as such recent developments as Generative Adversarial Networks and capsule networks. He also provides a comprehensive (and comprehensible) introduction to the two fundamental algorithms in deep learning: gradient descent and backpropagation. Finally, Kelleher considers the future of deep learning—major trends, possible developments, and significant challenges.}",
    isbn = {9780262354899},
    doi = {10.7551/mitpress/11171.001.0001},
    url = {https://doi.org/10.7551/mitpress/11171.001.0001},
}




@book{moolayil2019,
author = {Moolayil, Jojo},
year = {2019},
month = {01},
pages = {},
title = {Learn Keras for Deep Neural Networks: A Fast-Track Approach to Modern Deep Learning with Python},
isbn = {978-1-4842-4239-1},
doi = {10.1007/978-1-4842-4240-7},
publisher= {}
}

@article{naik2023,
  title={How to used Google Colab for Deep Learning using Python?},
  author={Naik, Prathamesh},
  year={2023},
  journal={All about Machine Learning},
  urldate={2024-07-16},
  url={https://blog.techcraft.org/how-to-used-google-colab-for-deep-learning-using-python/}
}

@INPROCEEDINGS{rajayogi2019,
  author={Rajayogi, J R and Manjunath, G and Shobha, G},
  booktitle={2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)}, 
  title={Indian Food Image Classification with Transfer Learning}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  keywords={Computational modeling;Image classification;Training;Machine learning;Convolution;Computer architecture;Google;Convolutional neural network;Google inception v3 model;VGG16;VGG19;ResNet;Transfer learning;Food classification Introduction (Heading 1)},
  doi={10.1109/CSITSS47250.2019.9031051}}


@book{rebala2019,
author = {Rebala, Gopinath and Ravi, Ajay and Churiwala, Sanjay},
year = {2019},
month = {01},
pages = {},
title = {An Introduction to Machine Learning},
isbn = {978-3-030-15728-9},
doi = {10.1007/978-3-030-15729-6},
publisher = {}
}

@misc{tan2020,
      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2020},
      eprint={1905.11946},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.11946}, 
}

@ARTICLE{tian2020,
  author={Tian, Youhui},
  journal={IEEE Access}, 
  title={Artificial Intelligence Image Recognition Method Based on Convolutional Neural Network Algorithm}, 
  year={2020},
  volume={8},
  number={},
  pages={125731-125744},
  keywords={Convolutional neural networks;Feature extraction;Image recognition;Convolution;Neurons;Recurrent neural networks;Optimization;Convolutional neural network;artificial intelligence;image recognition},
  doi={10.1109/ACCESS.2020.3006097}
}


@book{vasilev2019,
  title={Python Deep Learning, Second Edition},
  author={Vasilev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
  year={2019},
  publisher={Packt Publishing Ltd},
  address={Birmingham}
}


@article{yadav2022,
  author    = {Yadav, H.},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal   = {Journal of Machine Learning Research},
  volume    = {21},
  number    = {1},
  pages     = {1-30},
  year      = {2022}
}

@article{ye2022,
  author    = {Ye, Y. and others},
  title     = {An Improved EfficientNetV2 Model Based on Visual Attention Mechanism: Application to Identification of Cassava Disease},
  journal   = {Computational Intelligence and Neuroscience},
  volume    = {2022},
  year      = {2022},
  doi       = {10.1155/2022/1569911}
}

@article{Sarker2021,
   abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
   author = {Iqbal H Sarker},
   doi = {10.1007/s42979-021-00592-x},
   issn = {2661-8907},
   issue = {3},
   journal = {SN Computer Science},
   pages = {160},
   title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
   volume = {2},
   url = {https://doi.org/10.1007/s42979-021-00592-x},
   year = {2021},
}

@article{Namira2023, 
    title={HUBUNGAN PENGETAHUAN GIZI DAN PERILAKU MAKAN DENGAN STATUS GIZI SISWA SDN PUTAT JAYA II SURABAYA}, 
    volume={3}, 
    url={https://ejournal.unesa.ac.id/index.php/GIZIUNESA/article/view/50468}, 
    number={1}, 
    journal={GIZI UNESA}, 
    author={Namira, Nadila}, 
    year={2023}, 
    month={3}, 
    pages={215-222}
}

@article{Yamashita2018,
   abstract = {Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care.},
   author = {Rikiya Yamashita and Mizuho Nishio and Richard Kinh Gian Do and Kaori Togashi},
   doi = {10.1007/s13244-018-0639-9},
   issn = {1869-4101},
   issue = {4},
   journal = {Insights into Imaging},
   pages = {611-629},
   title = {Convolutional neural networks: an overview and application in radiology},
   volume = {9},
   url = {https://doi.org/10.1007/s13244-018-0639-9},
   year = {2018},
}

@misc{tan2021,
      title={EfficientNetV2: Smaller Models and Faster Training}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2021},
      eprint={2104.00298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.00298}, 
}

@book{andono2017,
  author = {Andono, Pulung Nurtantio and Sutojo, T. and Muljono},
  title = {Pengolahan citra digital},
  year = {2017},
  publisher = {ANDI},
  address = {Yogyakarta},
  editor = {Pramesta, Arie}
}

@book{bisong2019,
    author = {Bisong, Ekaba},
    year = {2019},
    month = {01},
    pages = {},
    title = {Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners},
    isbn = {978-1-4842-4469-2},
    doi = {10.1007/978-1-4842-4470-8},
    publisher = {Apress Berkeley, CA}
}

@book{setiawan2022,
  title={Praktek Pemrograman C++ dan Python},
  author={Setiawan, Gabriella Alicia and Vania, Evelyn},
  year={2022},
  publisher={SCU Knowledge Media},
  address={Jakarta},
  month={04}
}

@article{bopang2020,
author = {Bo Pang and Erik Nijkamp and Ying Nian Wu},
title ={Deep Learning With TensorFlow: A Review},
journal = {Journal of Educational and Behavioral Statistics},
volume = {45},
number = {2},
pages = {227-248},
year = {2020},
doi = {10.3102/1076998619872761},
URL = {https://doi.org/10.3102/1076998619872761},
eprint = {https://doi.org/10.3102/1076998619872761},
abstract = { This review covers the core concepts and design decisions of TensorFlow. TensorFlow, originally created by researchers at Google, is the most popular one among the plethora of deep learning libraries. In the field of deep learning, neural networks have achieved tremendous success and gained wide popularity in various areas. This family of models also has tremendous potential to promote data analysis and modeling for various problems in educational and behavioral sciences given its flexibility and scalability. We give the reader an overview of the basics of neural network models such as the multilayer perceptron, the convolutional neural network, and stochastic gradient descent, the most commonly used optimization method for neural network models. However, the implementation of these models and optimization algorithms is time-consuming and error-prone. Fortunately, TensorFlow greatly eases and accelerates the research and application of neural network models. We review several core concepts of TensorFlow such as graph construction functions, graph execution tools, and TensorFlow’s visualization tool, TensorBoard. Then, we apply these concepts to build and train a convolutional neural network model to classify handwritten digits. This review is concluded by a comparison of low- and high-level application programming interfaces and a discussion of graphical processing unit support, distributed training, and probabilistic modeling with TensorFlow Probability library. }
}

@article{chicho2021,
author = {Chicho, Bahzad and Sallow, Amira},
year = {2021},
month = {10},
pages = {},
title = {A Comprehensive Survey of Deep Learning Models Based on Keras Framework},
volume = {2},
journal = {Journal of Soft Computing and Data Mining},
doi = {10.30880/jscdm.2021.02.02.005}
}

@misc{bisaai2024scikit,
  title={Scikit-learn: AI For Everyone},
  author={{BISA AI}},
  year={2024},
  url={https://bisa.ai/course/detail/MzU3/1},
  urldate={2024-07-16},
  note={Accessed: 2024-07-16}
}

@misc{sun2019,
      title={A Survey of Optimization Methods from a Machine Learning Perspective}, 
      author={Shiliang Sun and Zehui Cao and Han Zhu and Jing Zhao},
      year={2019},
      eprint={1906.06821},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.06821}, 
}

@INPROCEEDINGS{bock2019,
  author={Bock, Sebastian and Weiß, Martin},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Proof of Local Convergence for the Adam Optimizer}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  keywords={Convergence;Dynamical systems;Optimization;Eigenvalues and eigenfunctions;Asymptotic stability;Neural networks;Training;Non-convex optimization;Adam optimizer;convergence;momentum method;dynamical system;fixed point},
  doi={10.1109/IJCNN.2019.8852239}}

@misc{zhuang2020,
      title={A Comprehensive Survey on Transfer Learning}, 
      author={Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
      year={2020},
      eprint={1911.02685},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.02685}, 
}

@misc{cmlabs2023,
  author = {{cmlabs}},
  title = {Flowchart: Definition, Functions, Types, and Symbols},
  year = {2023},
  url = {https://cmlabs.co/en-id/seo-guidelines/flowchart},
  urldate = {2024-07-16},
  note = {Accessed: 2024-07-16}
}

@INPROCEEDINGS{selvaraju2017,
  author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}, 
  year={2017},
  volume={},
  number={},
  pages={618-626},
  keywords={Visualization;Cats;Dogs;Computer architecture;Knowledge discovery},
  doi={10.1109/ICCV.2017.74}}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{ioffe2015,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167}, 
}

@misc{Jupyter2021,
  author       = {Jupyter},
  title        = {Jupyter Notebook},
  year         = {2021},
  url          = {https://jupyter.org},
  urldate      = {2024-07-31},
  note         = {Accessed: 2024-07-31}
}

@misc{Git2024,
  author       = {Git},
  title        = {About Git},
  year         = {2024},
  url          = {https://git-scm.com/about},
  urldate      = {2024-07-31},
  note         = {Accessed: 2024-07-31}
}

@book{Mitchell2018,
  author       = {Ryan Mitchell},
  title        = {Web Scraping with Python: Collecting More Data from the Modern Web},
  year         = {2018},
  edition      = {2nd},
  publisher    = {O'Reilly Media},
  isbn         = {978-1491985571}
}

@misc{Desai2019,
  author       = {Parul Desai},
  title        = {Learning Curve to Identify Overfitting \& Underfitting Problems},
  year         = {2019},
  url          = {https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5},
  urldate      = {2024-07-31},
  note         = {Accessed: 2024-07-31}
}

@misc{NVIDIA2021,
  author       = {NVIDIA Corporation},
  title        = {NVIDIA DGX Systems},
  year         = {2021},
  url          = {https://www.nvidia.com/en-us/data-center/dgx-systems/},
  urldate      = {2024-07-31},
  note         = {Accessed: 2024-07-31}
}

@misc{HPCGunadarma,
  author       = {HPC Hub Gunadarma},
  title        = {HPC Hub Universitas Gunadarma},
  year         = {2024},
  url          = {https://www.hpc-hub.gunadarma.ac.id/},
  urldate      = {2024-07-31},
  note         = {Accessed: 2024-07-31}
}

@article{fitria2022,
  title={Gambaran Tingkat Pengetahuan tentang Gizi Seimbang pada Siswa SMA Muhammadiyah 13 Jakarta},
  author={Fitria, Fitria and Musniati, Nia and Mulyawati, Devi Annisa},
  journal={Muhammadiyah Journal of Nutrition and Food Science (MJNF)},
  volume={3},
  number={1},
  pages={11--16},
  year={2022}
}



